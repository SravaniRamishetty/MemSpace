defaults:
  - base
  - dataset: replica
  - _self_

# Demo 2.2 specific settings
exp_name: demo_2_2_clip_embeddings
use_rerun: true

# Model settings
model:
  sam:
    model_type: mobile_sam
    device: ${device}
  clip:
    model_name: ViT-H-14
    pretrained: laion2b_s32b_b79k
    device: ${device}

# Dataset settings for this demo
dataset:
  stride: 50  # Larger stride for faster demo
  max_frames: 2  # Just 2 frames for quick demo
  start_frame: 0

# Segmentation settings (same as 2.1)
segmentation:
  min_mask_area: 100
  max_masks_per_frame: 50
  visualize_masks: true
  save_masks: false

  # Mask merging to reduce over-segmentation
  merge_masks: true
  merge_iou_threshold: 0.5
  merge_containment_threshold: 0.85

  # Optional: depth-based filtering
  filter_by_depth: false
  max_depth_variance: 0.5

# CLIP feature extraction settings
clip_features:
  padding: 20  # Padding around bounding boxes
  batch_size: 32  # Batch size for CLIP processing
  save_crops: false  # Save cropped images for visualization
  visualize_embeddings: true  # Show embedding space visualization
